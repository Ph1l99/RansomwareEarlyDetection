{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ransomware Early Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ransomware Early Detection based on pre-attack activities\n",
        "This notebook contains all the models the authors have developed."
      ],
      "metadata": {
        "id": "OpHXVN69rZjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we have to mount the Google Drive volume for dataset loading."
      ],
      "metadata": {
        "id": "XNTWGpb5r7h2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO-ph7KLkaRi",
        "outputId": "8a3b8a4d-6266-4331-b97b-9fccf13ea0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#ML auxiliary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Keras\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.metrics import TopKCategoricalAccuracy\n",
        "\n",
        "#SkLearn auxiliary libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "akznZb-Bkjzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the dataset and we reduce the number of samples per family (max. 450 items)."
      ],
      "metadata": {
        "id": "Up7MFFtSsCVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/dataset_cleaned_wo_blacklisted_families.csv')\n",
        "dataset = dataset.groupby('Family').head(450)\n",
        "family_count = dataset['Family'].sort_values()\n",
        "print(family_count.value_counts())"
      ],
      "metadata": {
        "id": "DkoUm81ck743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38f34a5-ef56-4b22-8f3d-6ba82b76f3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2     450\n",
            "5     450\n",
            "14    450\n",
            "15    450\n",
            "17    450\n",
            "23    446\n",
            "9     443\n",
            "0     432\n",
            "6     377\n",
            "3     359\n",
            "13    331\n",
            "8     295\n",
            "7     266\n",
            "Name: Family, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classification\n",
        "In this section we build the models for binary classification. The models we have chosen are:\n",
        "- Bernoulli Naive Bayes\n",
        "- K-Nearest Neighbors\n",
        "- Random Forest"
      ],
      "metadata": {
        "id": "rBIC9uCWsMRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preparation\n",
        "First of all we transform the dataset to a two-class one by replacing all families identifiers with *Ransomware* or *Benign* classes."
      ],
      "metadata": {
        "id": "ep9u44_PsWl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_class_dataset = dataset.copy(deep=True)\n",
        "binary_class_dataset['Family'] = np.where(binary_class_dataset.Family <= 22, 'Ransomware', 'Benign')"
      ],
      "metadata": {
        "id": "0NUQnQg6nIVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_class_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Su1T5WAoOg3",
        "outputId": "c1f9dacb-5555-41d4-a596-d1dbe28202da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5199, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then split the labels from the features in order to run the classification tasks."
      ],
      "metadata": {
        "id": "bOCtoySruCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = binary_class_dataset.to_numpy()\n",
        "x = data[:, :-1].astype(str)\n",
        "y = data[:, -1].astype(str)"
      ],
      "metadata": {
        "id": "NrcDBsS_ocp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Models\n",
        "In this section we train and validate the three models which have been already mentioned above for what concerns binary classification"
      ],
      "metadata": {
        "id": "V96qNns2scXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bernoulli Naive Bayes"
      ],
      "metadata": {
        "id": "txvyYDALsemX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bern_na_bay = BernoulliNB()\n",
        "\n",
        "scores = cross_val_score(bern_na_bay, x, y, cv=10)\n",
        "scores.mean()"
      ],
      "metadata": {
        "id": "dng-Le5Qoyby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b15ffb-7d69-4563-ff78-dd261094b212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9924929598340004"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "DFBAh0pMsg4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "scores = cross_val_score(knn, x, y, cv=10)\n",
        "scores.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8ZxnGh9p0PO",
        "outputId": "404d6e80-bc84-4d63-bdd6-26d4974dc61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9915314213724619"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "MWWmhB3jsj34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_for_classifier=RandomForestClassifier()\n",
        "\n",
        "scores = cross_val_score(rand_for_classifier, x, y, cv=10)\n",
        "scores.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0efW3nlwYxk",
        "outputId": "8e38f648-40d8-4b8b-9fe3-e003fd01263f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9990377204683563"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label classification"
      ],
      "metadata": {
        "id": "A721dsqhsQ4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preparation"
      ],
      "metadata": {
        "id": "4LavcEdq3iUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then split the labels from the features in order to run the classification tasks. We also remove the first row with column heading."
      ],
      "metadata": {
        "id": "dWScQBikz6Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.iloc[1: , :]\n",
        "data = dataset.to_numpy()\n",
        "x = data[:, :-1].astype(float)\n",
        "y = data[:, -1].astype(float)"
      ],
      "metadata": {
        "id": "q3ajmAEqulqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML model\n",
        "For multilabel classification we have chosen to use a Convolutional Neural Network"
      ],
      "metadata": {
        "id": "mLRKmkPk3nEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model creation and training\n",
        "y_cat = to_categorical(y)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_cat, test_size=0.2)\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=39, activation = \"relu\"))\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dense(128, activation = \"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(24, activation = \"softmax\"))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', TopKCategoricalAccuracy(k=2)])\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, verbose=1, epochs=100, batch_size=30)\n",
        "\n",
        "predict_x=model.predict(x_test) \n",
        "y_pred_class=np.argmax(predict_x,axis=1)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_test_class = np.argmax(y_test, axis=1)\n",
        "print(confusion_matrix(y_test_class, y_pred_class))\n",
        "\n",
        "print(classification_report(y_test_class, y_pred_class))"
      ],
      "metadata": {
        "id": "gWkh0-0Fuhr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with previous works"
      ],
      "metadata": {
        "id": "3lylDhfGheI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On Ransomware Family Attribution Using Pre-Attack Paranoia Activities"
      ],
      "metadata": {
        "id": "a7QlGBfAHVo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset loading\n",
        "paranoia_dataset = pd.read_csv('/content/drive/MyDrive/paranoia_dataset.csv')\n",
        "data_para = paranoia_dataset.to_numpy()\n",
        "xp = data_para[:, :-1].astype(float)\n",
        "yp = data_para[:, -1].astype(float)\n",
        "yp_cat = to_categorical(yp)\n",
        "xp_train, xp_test, yp_train, yp_test = train_test_split(xp, yp_cat, test_size=0.2)"
      ],
      "metadata": {
        "id": "PrRd5sH6HY5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first check the accuracy of their model using their dataset"
      ],
      "metadata": {
        "id": "a5HuUJtGIJ7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "randclf = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "randclf.fit(xp_train, yp_train)\n",
        "yp_train_pred = randclf.predict(xp_train)\n",
        "print('train precision: ' + str(precision_score(yp_train, yp_train_pred, average='weighted')))\n",
        "print('train recall: ' + str(recall_score(yp_train, yp_train_pred, average='weighted')))\n",
        "print('train accuracy: ' + str(accuracy_score(yp_train, yp_train_pred)))\n",
        "yp_test_pred = randclf.predict(xp_test)\n",
        "print('test precision: ' + str(precision_score(yp_test, yp_test_pred, average='weighted')))\n",
        "print('test recall: ' + str(recall_score(yp_test, yp_test_pred, average='weighted')))\n",
        "print('test accuracy: ' + str(accuracy_score(yp_test, yp_test_pred)))"
      ],
      "metadata": {
        "id": "RvoT6rFVHkyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use our model on their dataset to check the performances"
      ],
      "metadata": {
        "id": "MaPScY0oIQvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=23, activation = \"relu\"))\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dense(128, activation = \"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6, activation = \"softmax\"))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', TopKCategoricalAccuracy(k=2)])\n",
        "model.summary()\n",
        "model.fit(xp_train, yp_train, verbose=1, epochs=100, batch_size=30)\n",
        "\n",
        "predict_xp=model.predict(xp_test) \n",
        "yp_pred_class=np.argmax(predict_xp,axis=1)\n",
        "\n",
        "yp_pred = model.predict(xp_test)\n",
        "yp_test_class = np.argmax(yp_test, axis=1)\n",
        "\n",
        "print(classification_report(yp_test_class, yp_pred_class))"
      ],
      "metadata": {
        "id": "cYDv_ygQIEez"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}